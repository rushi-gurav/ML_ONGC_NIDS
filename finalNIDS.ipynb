{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddeed9e",
   "metadata": {},
   "source": [
    "# Step 1: Load and Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cebcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data files\n",
    "# Replace with correct paths in your environment\n",
    "files = [\n",
    "    'UNSW-NB15_1.csv', 'UNSW-NB15_2.csv',\n",
    "    'UNSW-NB15_3.csv', 'UNSW-NB15_4.csv'\n",
    "]\n",
    "features = pd.read_csv('NUSW-NB15_features.csv', encoding='ISO-8859-1')\n",
    "columns = features.iloc[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7f8bd",
   "metadata": {},
   "source": [
    "# Load and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545b46e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_90056\\1530396623.py:1: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_90056\\1530396623.py:1: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
    "merged_df.columns = columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1c95c",
   "metadata": {},
   "source": [
    "# Step 2: Filter to 9 attack categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "357f701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['id', 'srcip', 'sport', 'dstip', 'dsport', 'Label']\n",
    "df_cleaned = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "selected_attacks = [\n",
    "    'Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers',\n",
    "    'Generic', 'Reconnaissance', 'Shellcode', 'Worms'\n",
    "]\n",
    "df_cleaned = df_cleaned[df_cleaned['attack_cat'].isin(selected_attacks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd636d",
   "metadata": {},
   "source": [
    "# Step 3: Clean missing/duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c0116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_90056\\3349760250.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "df_cleaned.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e109911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_90056\\1251715571.py:16: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_90056\\1251715571.py:16: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_90056\\1251715571.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned.fillna(method='ffill', inplace=True)\n",
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [28] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:05:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Accuracy: 0.6639\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.52      0.47      0.49      8280\n",
      "      Backdoor       0.45      0.74      0.56      8279\n",
      "           DoS       0.48      0.08      0.14      8280\n",
      "      Exploits       0.57      0.62      0.59      8280\n",
      "       Generic       0.97      0.79      0.87      8280\n",
      "Reconnaissance       0.76      0.91      0.83      8279\n",
      "     Shellcode       0.86      0.75      0.80      8280\n",
      "         Worms       0.72      0.95      0.82      8280\n",
      "\n",
      "      accuracy                           0.66     66238\n",
      "     macro avg       0.67      0.66      0.64     66238\n",
      "  weighted avg       0.67      0.66      0.64     66238\n",
      "\n",
      "\n",
      "Random Forest Accuracy: 0.8189\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.57      0.59      0.58      8280\n",
      "      Backdoor       0.60      0.65      0.62      8279\n",
      "           DoS       0.68      0.61      0.64      8280\n",
      "      Exploits       0.83      0.84      0.83      8280\n",
      "       Generic       0.93      0.86      0.89      8280\n",
      "Reconnaissance       0.98      1.00      0.99      8279\n",
      "     Shellcode       0.99      1.00      1.00      8280\n",
      "         Worms       0.99      1.00      1.00      8280\n",
      "\n",
      "      accuracy                           0.82     66238\n",
      "     macro avg       0.82      0.82      0.82     66238\n",
      "  weighted avg       0.82      0.82      0.82     66238\n",
      "\n",
      "\n",
      "KNN Accuracy: 0.7904\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.52      0.68      0.59      8280\n",
      "      Backdoor       0.57      0.63      0.60      8279\n",
      "           DoS       0.64      0.60      0.62      8280\n",
      "      Exploits       0.86      0.59      0.70      8280\n",
      "       Generic       0.97      0.82      0.89      8280\n",
      "Reconnaissance       0.97      0.99      0.98      8279\n",
      "     Shellcode       0.98      1.00      0.99      8280\n",
      "         Worms       0.94      1.00      0.97      8280\n",
      "\n",
      "      accuracy                           0.79     66238\n",
      "     macro avg       0.81      0.79      0.79     66238\n",
      "  weighted avg       0.81      0.79      0.79     66238\n",
      "\n",
      "\n",
      "XGBoost Accuracy: 0.8328\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.65      0.57      0.61      8280\n",
      "      Backdoor       0.54      0.86      0.67      8279\n",
      "           DoS       0.88      0.52      0.65      8280\n",
      "      Exploits       0.83      0.86      0.84      8280\n",
      "       Generic       0.97      0.87      0.92      8280\n",
      "Reconnaissance       0.99      0.99      0.99      8279\n",
      "     Shellcode       0.99      1.00      0.99      8280\n",
      "         Worms       0.99      1.00      0.99      8280\n",
      "\n",
      "      accuracy                           0.83     66238\n",
      "     macro avg       0.85      0.83      0.83     66238\n",
      "  weighted avg       0.85      0.83      0.83     66238\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['new_label_encoder.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NIDS - Multi-class Classification on UNSW-NB15\n",
    "\n",
    "# Step 1: Load and Merge Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Data files\n",
    "# Replace with correct paths in your environment\n",
    "files = [\n",
    "    'UNSW-NB15_1.csv', 'UNSW-NB15_2.csv',\n",
    "    'UNSW-NB15_3.csv', 'UNSW-NB15_4.csv'\n",
    "]\n",
    "features = pd.read_csv('NUSW-NB15_features.csv', encoding='ISO-8859-1')\n",
    "columns = features.iloc[:, 1].tolist()\n",
    "\n",
    "# Load and merge\n",
    "merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
    "merged_df.columns = columns\n",
    "\n",
    "# Step 2: Filter to 9 attack categories\n",
    "columns_to_drop = ['id', 'srcip', 'sport', 'dstip', 'dsport', 'Label']\n",
    "df_cleaned = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "selected_attacks = [\n",
    "    'Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers',\n",
    "    'Generic', 'Reconnaissance', 'Shellcode', 'Worms'\n",
    "]\n",
    "df_cleaned = df_cleaned[df_cleaned['attack_cat'].isin(selected_attacks)]\n",
    "\n",
    "# Step 3: Clean missing/duplicates\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "df_cleaned.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Step 4: Encode categorical and scale\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['attack_cat'] = label_encoder.fit_transform(df_cleaned['attack_cat'])\n",
    "\n",
    "X = df_cleaned.drop(columns=['attack_cat'])\n",
    "y = df_cleaned['attack_cat']\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5: Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=40)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Step 6: Balance dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_selected, y)\n",
    "\n",
    "# Step 7: Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Step 8: Train models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "\n",
    "# Step 9: Evaluate\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Step 10: Save model\n",
    "import joblib\n",
    "joblib.dump(trained_models['XGBoost'], 'best_nids_model.pkl')\n",
    "joblib.dump(label_encoder, 'new_label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa144bbc",
   "metadata": {},
   "source": [
    "Step 1: Load and Merge UNSW-NB15 Dataset\n",
    "### Step 1: Load and Merge Dataset\n",
    "\n",
    "We are loading the 4 parts of the UNSW-NB15 dataset and assigning appropriate column names using the feature file. This gives us a complete dataset of ~2.5 million rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd5e1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25752\\1658876593.py:11: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25752\\1658876593.py:11: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Merged dataset shape: (2540047, 49)\n",
      "ðŸ“Œ First 5 rows:\n",
      "        srcip  sport          dstip dsport proto state       dur  sbytes  \\\n",
      "0  59.166.0.0   1390  149.171.126.6     53   udp   CON  0.001055     132   \n",
      "1  59.166.0.0  33661  149.171.126.9   1024   udp   CON  0.036133     528   \n",
      "2  59.166.0.6   1464  149.171.126.7     53   udp   CON  0.001119     146   \n",
      "3  59.166.0.5   3593  149.171.126.5     53   udp   CON  0.001209     132   \n",
      "4  59.166.0.3  49664  149.171.126.0     53   udp   CON  0.001169     146   \n",
      "\n",
      "   dbytes  sttl  ...  ct_ftp_cmd  ct_srv_src  ct_srv_dst ct_dst_ltm  \\\n",
      "0     164    31  ...           0           3           7          1   \n",
      "1     304    31  ...           0           2           4          2   \n",
      "2     178    31  ...           0          12           8          1   \n",
      "3     164    31  ...           0           6           9          1   \n",
      "4     178    31  ...           0           7           9          1   \n",
      "\n",
      "   ct_src_ ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \\\n",
      "0            3                 1                 1               1   \n",
      "1            3                 1                 1               2   \n",
      "2            2                 2                 1               1   \n",
      "3            1                 1                 1               1   \n",
      "4            1                 1                 1               1   \n",
      "\n",
      "   attack_cat  Label  \n",
      "0         NaN      0  \n",
      "1         NaN      0  \n",
      "2         NaN      0  \n",
      "3         NaN      0  \n",
      "4         NaN      0  \n",
      "\n",
      "[5 rows x 49 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load feature names\n",
    "features = pd.read_csv('NUSW-NB15_features.csv', encoding='ISO-8859-1')\n",
    "columns = features.iloc[:, 1].tolist()\n",
    "\n",
    "# List of dataset files\n",
    "files = ['UNSW-NB15_1.csv', 'UNSW-NB15_2.csv', 'UNSW-NB15_3.csv', 'UNSW-NB15_4.csv']\n",
    "\n",
    "# Load and merge all parts\n",
    "merged_df = pd.concat([pd.read_csv(f, header=None) for f in files], ignore_index=True)\n",
    "merged_df.columns = columns\n",
    "\n",
    "# Dataset summary\n",
    "print(\"âœ… Merged dataset shape:\", merged_df.shape)\n",
    "print(\"ðŸ“Œ First 5 rows:\")\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8447bafc",
   "metadata": {},
   "source": [
    "### Step 2: Filter to 9 Attack Categories\n",
    "\n",
    "We are keeping only the attack categories that we are interested in and dropping irrelevant network-identifying columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf58e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset shape after filtering attack categories: (282987, 44)\n",
      "ðŸ“Š Attack category counts:\n",
      "attack_cat\n",
      "Generic           215481\n",
      "Exploits           44525\n",
      "DoS                16353\n",
      "Analysis            2677\n",
      "Backdoor            1795\n",
      "Reconnaissance      1759\n",
      "Shellcode            223\n",
      "Worms                174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop unwanted columns\n",
    "columns_to_drop = ['id', 'srcip', 'sport', 'dstip', 'dsport', 'Label']\n",
    "df_cleaned = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Keep only 9 relevant attack types\n",
    "selected_attacks = [\n",
    "    'Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers',\n",
    "    'Generic', 'Reconnaissance', 'Shellcode', 'Worms'\n",
    "]\n",
    "df_cleaned = df_cleaned[df_cleaned['attack_cat'].isin(selected_attacks)]\n",
    "\n",
    "print(\"âœ… Dataset shape after filtering attack categories:\", df_cleaned.shape)\n",
    "print(\"ðŸ“Š Attack category counts:\")\n",
    "print(df_cleaned['attack_cat'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38750084",
   "metadata": {},
   "source": [
    "### Step 3: Handle Duplicates and Missing Values\n",
    "\n",
    "We remove duplicate rows and fill any missing values using forward fill (`ffill`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da661d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Shape after cleaning missing/duplicate rows: (64551, 44)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_25752\\375475971.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_cleaned.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "df_cleaned.drop_duplicates(inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "df_cleaned.fillna(method='ffill', inplace=True)\n",
    "\n",
    "print(\"âœ… Shape after cleaning missing/duplicate rows:\", df_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caa5a2",
   "metadata": {},
   "source": [
    "### Step 4: Encode Target and Scale Features\n",
    "\n",
    "We:\n",
    "- Encode `attack_cat` using `LabelEncoder`\n",
    "- Use one-hot encoding for any remaining categorical features\n",
    "- Scale all numerical features using `StandardScaler`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c203d9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature matrix shape (after encoding and scaling): (64551, 194)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Encode target\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['attack_cat'] = label_encoder.fit_transform(df_cleaned['attack_cat'])\n",
    "\n",
    "# Split features and labels\n",
    "X = df_cleaned.drop(columns=['attack_cat'])\n",
    "y = df_cleaned['attack_cat']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"âœ… Feature matrix shape (after encoding and scaling):\", X_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da027e6",
   "metadata": {},
   "source": [
    "### Step 5: Feature Selection\n",
    "\n",
    "We select the top 40 most relevant features using ANOVA F-test with `SelectKBest`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60267b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Shape after selecting top 40 features: (64551, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [28] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=40)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "print(\"âœ… Shape after selecting top 40 features:\", X_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample from your preprocessed dataset\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load what your model used\n",
    "X_selected = pd.DataFrame(X_selected)  # assuming it's already your 40-feature matrix\n",
    "\n",
    "# Save one row\n",
    "X_selected.iloc[0].to_csv(\"sample_input.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d264a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong way (vertical - 40x1):\n",
    "X_selected.iloc[0].to_csv(\"sample_input.csv\", index=False, header=False)\n",
    "\n",
    "# âœ… Correct way (horizontal - 1x40):\n",
    "X_selected.iloc[[0]].to_csv(\"sample_input.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10876b0",
   "metadata": {},
   "source": [
    "### Step 6: Balance Dataset Using SMOTE\n",
    "\n",
    "We balance the number of samples across all attack categories using SMOTE (Synthetic Minority Over-sampling Technique).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d11c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resampled shape: (220792, 40)\n",
      "ðŸ“Š Balanced class distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "attack_cat\n",
       "3    27599\n",
       "5    27599\n",
       "2    27599\n",
       "4    27599\n",
       "6    27599\n",
       "7    27599\n",
       "0    27599\n",
       "1    27599\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_selected, y)\n",
    "\n",
    "print(\"âœ… Resampled shape:\", X_resampled.shape)\n",
    "print(\"ðŸ“Š Balanced class distribution:\")\n",
    "pd.Series(y_resampled).value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593dc0a",
   "metadata": {},
   "source": [
    "### Step 7: Split Data into Train and Test Sets\n",
    "\n",
    "We split the data into 70% training and 30% testing with stratification to preserve class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8014708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training set shape: (154554, 40), Testing set shape: (66238, 40)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "print(f\"âœ… Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252e5a8",
   "metadata": {},
   "source": [
    "### Step 8: Train Multiple Models\n",
    "\n",
    "We train the following classifiers:\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- K-Nearest Neighbors\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d18002da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trained: Logistic Regression\n",
      "âœ… Trained: Random Forest\n",
      "âœ… Trained: KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ONGC_ML\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:46:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Trained: XGBoost\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=500),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    print(f\"âœ… Trained: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae3f2bf",
   "metadata": {},
   "source": [
    "### Step 9: Evaluate Trained Models\n",
    "\n",
    "We evaluate each model using accuracy and the classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf16362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Logistic Regression Accuracy: 0.6639\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.52      0.47      0.49      8280\n",
      "      Backdoor       0.45      0.74      0.56      8279\n",
      "           DoS       0.48      0.08      0.14      8280\n",
      "      Exploits       0.57      0.62      0.59      8280\n",
      "       Generic       0.97      0.79      0.87      8280\n",
      "Reconnaissance       0.76      0.91      0.83      8279\n",
      "     Shellcode       0.86      0.75      0.80      8280\n",
      "         Worms       0.72      0.95      0.82      8280\n",
      "\n",
      "      accuracy                           0.66     66238\n",
      "     macro avg       0.67      0.66      0.64     66238\n",
      "  weighted avg       0.67      0.66      0.64     66238\n",
      "\n",
      "\n",
      "ðŸ“ˆ Random Forest Accuracy: 0.8193\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.57      0.59      0.58      8280\n",
      "      Backdoor       0.60      0.66      0.63      8279\n",
      "           DoS       0.68      0.61      0.64      8280\n",
      "      Exploits       0.83      0.84      0.83      8280\n",
      "       Generic       0.93      0.86      0.89      8280\n",
      "Reconnaissance       0.98      1.00      0.99      8279\n",
      "     Shellcode       0.99      1.00      1.00      8280\n",
      "         Worms       0.99      1.00      1.00      8280\n",
      "\n",
      "      accuracy                           0.82     66238\n",
      "     macro avg       0.82      0.82      0.82     66238\n",
      "  weighted avg       0.82      0.82      0.82     66238\n",
      "\n",
      "\n",
      "ðŸ“ˆ KNN Accuracy: 0.7904\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.52      0.68      0.59      8280\n",
      "      Backdoor       0.57      0.63      0.60      8279\n",
      "           DoS       0.64      0.60      0.62      8280\n",
      "      Exploits       0.86      0.59      0.70      8280\n",
      "       Generic       0.97      0.82      0.89      8280\n",
      "Reconnaissance       0.97      0.99      0.98      8279\n",
      "     Shellcode       0.98      1.00      0.99      8280\n",
      "         Worms       0.94      1.00      0.97      8280\n",
      "\n",
      "      accuracy                           0.79     66238\n",
      "     macro avg       0.81      0.79      0.79     66238\n",
      "  weighted avg       0.81      0.79      0.79     66238\n",
      "\n",
      "\n",
      "ðŸ“ˆ XGBoost Accuracy: 0.8328\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.65      0.57      0.61      8280\n",
      "      Backdoor       0.54      0.86      0.67      8279\n",
      "           DoS       0.88      0.52      0.65      8280\n",
      "      Exploits       0.83      0.86      0.84      8280\n",
      "       Generic       0.97      0.87      0.92      8280\n",
      "Reconnaissance       0.99      0.99      0.99      8279\n",
      "     Shellcode       0.99      1.00      0.99      8280\n",
      "         Worms       0.99      1.00      0.99      8280\n",
      "\n",
      "      accuracy                           0.83     66238\n",
      "     macro avg       0.85      0.83      0.83     66238\n",
      "  weighted avg       0.85      0.83      0.83     66238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nðŸ“ˆ {name} Accuracy: {acc:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8952f45",
   "metadata": {},
   "source": [
    "### Step 10: Save the Best Model\n",
    "\n",
    "We save the XGBoost model and label encoder for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2469c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and label encoder saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(trained_models['XGBoost'], 'best_nids_model.pkl')\n",
    "joblib.dump(label_encoder, 'new_label_encoder.pkl')\n",
    "print(\"âœ… Model and label encoder saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
